{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab44edb",
   "metadata": {},
   "source": [
    "# Variational AutoEncode using pytorch on MNIST\n",
    "\n",
    "This notebook is to illustrate the conept of Variational Auto-Encoder on MNIST data. VAE is a unsupervised (or semi-supervised) technique.\n",
    "\n",
    "The basic principle is that using an Encoding NN the input data features are reduced into a latent space, and then with another Decoder netword the opposite happens and the decoding network learns to create the original image from the latent space.\n",
    "\n",
    "The applications could be creating similar outputs to the original dataset that have not been created before. Thus it can have applications in generative or data augmentation application.\n",
    "\n",
    "Also a very powerful application could be in de-noising. Noise can be treated as higher frequency data that will be filter as we go to the latent space. This can be a powerful technique in image processing.\n",
    "\n",
    "However, this notebook is only showing how the VAE can result in simple classification. Similar to the previous notebook on MNIST numbers for dimension reduction, a VAE is created to reduce the dimension to 2 for the 28x28 images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5ab94",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb9857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fb6528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device is cuda\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(f\"Available device is {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c5a9a",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Instead of reading the data from the csv file, that could be painful to transform into suitable format for dataloader, it is read from torch datasets.\n",
    "\n",
    "Torch has a dataloader that can take care of batch creation, it works and we don't need to make a data-generator. We read the data and send it to the dataloader that will be called during training and validation.\n",
    "\n",
    "Data visualization is skipped as it was done in the other notebook on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62eb8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "batch_size = 100\n",
    "train_dataset = torchvision.datasets.MNIST('~/datasets', transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST('~/datasets', transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)#, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False)#, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d17e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim  = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd208a3",
   "metadata": {},
   "source": [
    "# Creating the encode and decoder\n",
    "\n",
    "First encoder is created. \n",
    "\n",
    "nn.Module is base class for all torch nn model and our models should extend (inherit from) this class. [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "The encode model has 4 dense (linear layers):\n",
    "- First layer has dimensions of x_dim x h_dim\n",
    "- Second layer has dimensions of h_dim x h_dim\n",
    "- Third layer has dimensions of h_dum x l_dim\n",
    "\n",
    "The model has two vector output with dimension of the latent space (l_dim). One is to learn a mean and the other one is to learn the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47dfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 400\n",
    "l_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae427a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim, l_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_l1 = nn.Linear(x_dim, h_dim)\n",
    "        self.FC_l2 = nn.Linear(h_dim, h_dim)\n",
    "        self.FC_latent_mean  = nn.Linear(h_dim, l_dim)\n",
    "        self.FC_latent_var   = nn.Linear (h_dim, l_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.FC_l1(x))\n",
    "        h_       = self.LeakyReLU(self.FC_l2(h_))\n",
    "        mean     = self.FC_latent_mean(h_)\n",
    "        log_var  = self.FC_latent_var(h_)                 \n",
    "                                                       \n",
    "        \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b86e1",
   "metadata": {},
   "source": [
    "The decoder simply does the opposite of the encode and has layers to transform the latent space back into the input data space. So it does not care about variations. \n",
    "\n",
    "It returns the sigmoid of the output layer with dimensions of input and it will be used when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e429555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, l_dim, h_dim, x_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(l_dim, h_dim)\n",
    "        self.FC_hidden2 = nn.Linear(h_dim, h_dim)\n",
    "        self.FC_output = nn.Linear(h_dim, x_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792690b2",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6bafe",
   "metadata": {},
   "source": [
    "To create the model the encode and decoder are passed to the final model.\n",
    "\n",
    "The model takes the input, passes that through the encoder> The encoder returns the learnt latent space and the learnt log of the variance in the latent space (We will see how later). So the encoder output are two vectors of:\n",
    "$$\n",
    " [\\mu_{1}, \\mu_{2},... , \\mu_{n}]\n",
    "$$\n",
    "and\n",
    "$$\n",
    " [log(\\sigma^2_{1}), log(\\sigma^2_{2}),... , log(\\sigma^2_{n})]\n",
    "$$\n",
    "\n",
    "where $\\sigma_{i}$ is the variance of the ith dimension of the latent space from the training dataset that we will give to it.\n",
    "\n",
    "The model will modify the latent space to $\\mu_{i} + \\epsilon \\sigma_{i}$ for all i in the latent space. Where $\\epsilon$ is a guassian random number between 0 and 1. The ideas to create a random latent space based on the input but the randomness is guassian and in the variance is the same as the learn variance.\n",
    "\n",
    "So the model creates a latent space vector as below from the inputs of the encoder. \n",
    "This techniques is refered to as reparametrizationi so that the derivates can still be calculated independent from the randomness.\n",
    "\n",
    "$$\n",
    " [\\mu_{1} + \\epsilon_{1}\\sigma_{1}, \\mu_{2} + \\epsilon_{1}\\sigma_{1},... , \\mu_{n} + + \\epsilon_{n}\\sigma_{n}]\n",
    "$$\n",
    "\n",
    "And then it sends it to the decoder to create the output in the input data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)           \n",
    "        z = mean + var*epsilon                          \n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) \n",
    "        x_hat            = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d387ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(x_dim, h_dim, l_dim)\n",
    "decoder = Decoder(l_dim, h_dim, x_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9879bdd",
   "metadata": {},
   "source": [
    "## Defining the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ab49a",
   "metadata": {},
   "source": [
    "The goal of the decoder is to calculate the probability distribution of\n",
    "\n",
    "The encoder calculates the probability of the latent space given an input x\n",
    "\n",
    "$$\n",
    "P(z|x) = \\frac{P(x|z)P(z)}{P(x)}\\\n",
    "$$\n",
    "\n",
    "Calculating this is not possible because $P(x)$ is not known and can only be presented as $\\Sigma_{z}P(x|z)P(z)$\n",
    "The idea is to find another known distribution, Q(z), as close as possible by minimize the KL divergence between P(z|x) and Q(z).\n",
    "\n",
    "$$\n",
    "min KL(P(z|x) || Q(z)) = -\\Sigma_{z}Q(z)log(\\frac{P(z|x)}{Q(z)})\n",
    "$$\n",
    "\n",
    "Using the first equation:\n",
    "\n",
    "$$\n",
    "min KL(P(z|x) || Q(z)) = -\\Sigma_{z}Q(z)log(\\frac{\\frac{P(x,z)}{P(x)}}{Q(z)}) = -\\Sigma_{z}Q(z)log(\\frac{P(x,z)}{Q(z)}) + \\Sigma_{z}Q(z)logP(x) = -\\Sigma_{z}Q(z)log(\\frac{P(x,z)}{Q(z)}) + logP(x)\n",
    "$$\n",
    "\n",
    "The term $\\Lambda = \\Sigma_{z}Q(z)log(\\frac{P(x,z)}{Q(z)})$ is called variational lower bound. Instead the optimization process could aim to maximize this term. It is lower bound because since KL is always positive $\\Lambda <= logP(x)$\n",
    "\n",
    "Writing the equations now for $\\Lambda $:\n",
    "\n",
    "$$\n",
    "\\Lambda = \\Sigma_{z}Q(z)log(\\frac{P(x,z)}{Q(z)}) = \\Sigma_{z}Q(z)log(\\frac{P(x|z)P(z)}{Q(z)}) = \\Sigma_{z}Q(z)logP(x|z) + \\Sigma_{z}Q(z)log(\\frac{P(z)}{Q(z)})\n",
    "$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$\n",
    "\\Lambda = \\Sigma_{z}Q(z)logP(x|z) - KL(Q(z)|| P(z)) = \\Sigma_{z}Q(z)logP(x|\\hat{x}) - KL(Q(z)|| P(z))\n",
    "$$\n",
    "\n",
    "Encoder gives Q(z|x) for input x, and the decoder output P(x|z) given a laten space z. Since the decoder part is deterministic we can also write it as $P(x|\\hat{x})$\n",
    "\n",
    "For the loss function as above we calculate the first part $\\Sigma_{z}Q(z)logP(x|\\hat{x})$ as the cross-entropy between the input and output layer. This is called the reproduction cost.\n",
    "\n",
    "The KL term is KL between two guasian distribution. One is Q(z) which is N(0, 1) (for all the dimensions in the latent space) and for the other one mean and variance are given as the outputs of the encoder. The KL between two guassians as calculate exactly in [Link](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians)\n",
    "\n",
    "Since we have chosen the encoder to output the logVariance instead of variance or standard deviation the calculations will also be based on that.\n",
    "\n",
    "$$\n",
    "KL (p(\\mu_1, \\sigma_1) | q(\\mu_2, \\sigma_2)) = log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma^2_1 + (\\mu_1 - \\mu_2)^2}{2\\sigma^2_2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu1 = mean\n",
    "$$\n",
    "$$\n",
    "\\sigma1 = exp(0.5 * logvar) \n",
    "$$\n",
    "$$\n",
    "\\mu2 = 0\n",
    "$$\n",
    "$$\n",
    "\\sigma2 = 1\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "KL (p(\\mu_1, \\sigma_1) | q(\\mu_2, \\sigma_2)) = log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma^2_1 + (\\mu_1 - \\mu_2)^2}{2\\sigma^2_2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL (p(\\mu_1, \\sigma_1) | q(\\mu_2, \\sigma_2)) = log\\frac{1}{exp(0.5 * logvar)} + \\frac{(exp(0.5 * logvar))^2  + (mean)^2}{2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL (p(\\mu_1, \\sigma_1) | q(\\mu_2, \\sigma_2)) = -0.5 * logvar + \\frac{exp(logvar)  + (mean)^2}{2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "can be summerized as:\n",
    "\n",
    "$$\n",
    "KL (p(\\mu_1, \\sigma_1) | q(\\mu_2, \\sigma_2)) = \\frac{-logvar + exp(logvar)  + (mean)^2 - 1}{2}\n",
    "$$\n",
    "\n",
    "And this has to be summed on all the dimensions of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93674d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "lr = 1e-3\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KL      = torch.sum(-log_var + log_var.exp() + mean.pow(2) - 1) / 2\n",
    "    \n",
    "    return reproduction_loss + KL\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9a55",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae00a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebb585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Variational AutoEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:08<00:00, 70.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 complete! \tLoss:  180.5158416534067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:07<00:00, 78.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 2 complete! \tLoss:  163.883365553918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:07<00:00, 79.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 3 complete! \tLoss:  159.47743804778798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:07<00:00, 80.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 4 complete! \tLoss:  156.97576700099123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:07<00:00, 79.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 5 complete! \tLoss:  155.22565221006366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 487/600 [00:06<00:01, 76.81it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Training the Variational AutoEncoder\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(tqdm((train_loader))):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tLoss: \", overall_loss / (batch_idx * batch_size))\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0cb3d",
   "metadata": {},
   "source": [
    "# Visualizing the result\n",
    "\n",
    "First we send the test set, that was not seen by the model during the training, and plot the latent space that is for now chosen to be 2, in a 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9363b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992010fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x, idx):\n",
    "    x = x.view(batch_size, 28, 28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(x[idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26873b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "x_arr = []\n",
    "y_arr = []\n",
    "l_arr = []\n",
    "with torch.no_grad():\n",
    "    for (x, l) in tqdm(test_loader):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "        x_hat, _ = encoder(x)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            x_y = x_hat[i].cpu().detach().numpy()\n",
    "            x_arr.append(x_y[0])\n",
    "            y_arr.append(x_y[1])\n",
    "            \n",
    "            label = l[i].detach().numpy()\n",
    "            l_arr.append(label)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf10514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "colors = ['red','green','blue','purple', 'black', 'orange', 'brown', 'olive', 'gray', 'pink']\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_arr, y_arr, c=l_arr, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a909406",
   "metadata": {},
   "source": [
    "Then we sent the training dataset again through the model and store the latent space for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebff193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x_arr = []\n",
    "y_arr = []\n",
    "l_arr = []\n",
    "with torch.no_grad():\n",
    "    for (x, l) in tqdm(train_loader):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "        x_hat, _ = encoder(x)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            x_y = x_hat[i].cpu().detach().numpy()\n",
    "            x_arr.append(x_y[0])\n",
    "            y_arr.append(x_y[1])\n",
    "            \n",
    "            label = l[i].detach().numpy()\n",
    "            l_arr.append(label)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ae682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "colors = ['red','green','blue','purple', 'black', 'orange', 'brown', 'olive', 'gray', 'pink']\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_arr, y_arr, c=l_arr, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524f457",
   "metadata": {},
   "source": [
    "The encoding part can be used for dimension reduction. The performance seem better than PCA. In principle if we did not have the non-linearities the network would converge to what PCA does.\n",
    "\n",
    "The decoding part can be used for generative tasks. We can take a sample from the latent space, add some noise according the calculated variance, this is called sampling and extract the output of a decoter as a similar but new data. Applications could be in data augmentation, new game character or art generation etc.\n",
    "\n",
    "Here to illustrate this we sample as the 2D latent space from -4 to +4 in both direction in 0.8 steps and check how the outputs look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82576788",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = []\n",
    "\n",
    "for i in np.arange(-3, 3, .6):\n",
    "    for j in np.arange(-3, 3, 0.6):\n",
    "        input_.append(np.array([i, j]))\n",
    "\n",
    "input_ = np.array(input_)\n",
    "input_ = torch.from_numpy(input_).float().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_images = decoder(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_images)\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        img = generated_images[j * 10 + i]\n",
    "        img = img.cpu().detach().numpy()\n",
    "        img = img.reshape(-1, 28)\n",
    "        plt.subplot(10, 10, j * 10 + i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086bb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
